# An Empirical Study of Ensemble Techniques (Bagging, Boosting and Stacking
In this repo, I present a research work on ensemble techniques. The abstract is presented below. 

# Abstract

Ensemble  methods are  popular  strategies for improving the  predictive ability  of a machine learning  model.  An ensemble consists  of  a  set of  individually  trained base learners/models whose  predictions  are  combined  when  classifying  new  cases. Previous researches have  shown that an ensemble is  on the  average  more  accurate than a  single base  model. Bagging, Boosting and  Stacking  are  some  popular  ensemble  techniques which we  studied  in this paper. We  evaluated  these  ensembles  on 9  data  sets. From our results, we  observed the following. First,  an ensemble is always more  accurate than a single  base  model.  Secondly, we  observed that Boosting ensembles is  on the average better than Bagging while  Stacking  (meta-learning)  is on  the average  more  accurate than Boosting and  Bagging. Further experiment also shows
that  the gain in predictive  power of  any  ensembles may  sometimes be small  or even  decrease  depending on the  data set.

Link to full paper [here](
https://www.academia.edu/39060549/An_Empirical_Study_of_Ensemble_Techniques_Bagging_Boosting_and_Stacking?source=swp_share)

Link to notebook [here](https://github.com/risenW/empirical-study-of-ensembles/blob/master/Empirical%20Ensemble%20Study%20By%20Rising%20Odegua.ipynb)

Link to Poster [here](https://github.com/risenW/empirical-study-of-ensembles/blob/master/POSTER%20Empirical%20Ensemble%20Study%20By%20Rising%20Odegua.pdf)

Leave a star if this work helps you. (smile)For questions, suggestions, corrections or collaborations don't hesitate to contact me.
